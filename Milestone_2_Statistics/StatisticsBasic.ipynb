{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0095aa1",
   "metadata": {},
   "source": [
    "# Statistics Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99b1ff",
   "metadata": {},
   "source": [
    "## 1. What is statistics, and why is it important?\n",
    "Statistics is the science of collecting, organizing, analyzing, interpreting, and presenting data. It's important because:\n",
    "- Helps make informed decisions based on data\n",
    "- Enables prediction of future trends\n",
    "- Provides tools for testing hypotheses\n",
    "- Essential for scientific research and business analytics\n",
    "- Helps understand uncertainty and variability in data\n",
    "\n",
    "## 2. What are the two main types of statistics?\n",
    "The two main types are:\n",
    "1. **Descriptive Statistics**: Summarizes and describes data\n",
    "2. **Inferential Statistics**: Makes predictions or inferences about a population based on sample data\n",
    "\n",
    "## 3. What are descriptive statistics?\n",
    "Descriptive statistics are methods used to summarize and describe the main features of a dataset. Examples include:\n",
    "- Measures of central tendency (mean, median, mode)\n",
    "- Measures of variability (range, variance, standard deviation)\n",
    "- Graphical representations (histograms, box plots)\n",
    "\n",
    "## 4. What is inferential statistics?\n",
    "Inferential statistics uses sample data to make generalizations about a larger population. It includes:\n",
    "- Hypothesis testing\n",
    "- Confidence intervals\n",
    "- Regression analysis\n",
    "- ANOVA\n",
    "\n",
    "## 5. What is sampling in statistics?\n",
    "Sampling is the process of selecting a subset of individuals from a population to estimate characteristics of the whole population. It's done because studying the entire population is often impractical.\n",
    "\n",
    "## 6. What are the different types of sampling methods?\n",
    "Main sampling methods:\n",
    "- **Probability Sampling**:\n",
    "  - Simple random sampling\n",
    "  - Stratified sampling\n",
    "  - Cluster sampling\n",
    "  - Systematic sampling\n",
    "- **Non-Probability Sampling**:\n",
    "  - Convenience sampling\n",
    "  - Purposive sampling\n",
    "  - Quota sampling\n",
    "  - Snowball sampling\n",
    "\n",
    "## 7. What is the difference between random and non-random sampling?\n",
    "**Random Sampling**:\n",
    "- Every member has known, non-zero chance of selection\n",
    "- Reduces bias\n",
    "- Allows for statistical inference\n",
    "\n",
    "**Non-Random Sampling**:\n",
    "- Selection based on convenience or judgment\n",
    "- May introduce bias\n",
    "- Limits generalizability\n",
    "\n",
    "## 8. Define and give examples of qualitative and quantitative data\n",
    "**Qualitative Data** (Categorical):\n",
    "- Describes qualities/characteristics\n",
    "- Examples: Gender, color, satisfaction level\n",
    "\n",
    "**Quantitative Data** (Numerical):\n",
    "- Can be measured numerically\n",
    "- Examples: Height, weight, temperature\n",
    "\n",
    "## 9. What are the different types of data in statistics?\n",
    "Four measurement scales:\n",
    "1. **Nominal**: Categories without order (e.g., colors)\n",
    "2. **Ordinal**: Ordered categories (e.g., satisfaction levels)\n",
    "3. **Interval**: Equal intervals, no true zero (e.g., temperature in °C)\n",
    "4. **Ratio**: Equal intervals with true zero (e.g., height, weight)\n",
    "\n",
    "## 10. Explain nominal, ordinal, interval, and ratio levels of measurement\n",
    "- **Nominal**: Categories without mathematical meaning (e.g., gender, race)\n",
    "- **Ordinal**: Ordered categories where difference between values isn't meaningful (e.g., Likert scales)\n",
    "- **Interval**: Equal intervals between values, no true zero (e.g., temperature in Celsius)\n",
    "- **Ratio**: Equal intervals with true zero point (e.g., height, weight, age)\n",
    "\n",
    "## 11. What is the measure of central tendency?\n",
    "Measures of central tendency describe the center or typical value of a dataset. The three main measures are:\n",
    "- Mean\n",
    "- Median\n",
    "- Mode\n",
    "\n",
    "## 12. Define mean, median, and mode\n",
    "- **Mean**: The average (sum of all values divided by number of values)\n",
    "- **Median**: The middle value when data is ordered\n",
    "- **Mode**: The most frequently occurring value\n",
    "\n",
    "## 13. What is the significance of the measure of central tendency?\n",
    "Significance:\n",
    "- Provides a single value representing the entire dataset\n",
    "- Helps compare different datasets\n",
    "- Serves as basis for many statistical analyses\n",
    "- Gives first impression of data distribution\n",
    "\n",
    "## 14. What is variance, and how is it calculated?\n",
    "Variance measures how far each number in a dataset is from the mean. Calculation:\n",
    "1. Find the mean\n",
    "2. Subtract mean from each data point and square the result\n",
    "3. Average these squared differences\n",
    "\n",
    "Formula: σ² = Σ(xᵢ - μ)²/N\n",
    "\n",
    "## 15. What is standard deviation, and why is it important?\n",
    "Standard deviation is the square root of variance. It's important because:\n",
    "- Measures dispersion in same units as original data\n",
    "- Indicates how spread out data is\n",
    "- Helps identify outliers\n",
    "- Fundamental for many statistical tests\n",
    "\n",
    "## 16. Define and explain the term range in statistics\n",
    "Range is the difference between the highest and lowest values in a dataset. It's the simplest measure of dispersion but sensitive to outliers.\n",
    "\n",
    "## 17. What is the difference between variance and standard deviation?\n",
    "- **Variance**: Average of squared deviations from mean (units squared)\n",
    "- **Standard Deviation**: Square root of variance (original units)\n",
    "- SD is more interpretable as it's in same units as data\n",
    "\n",
    "## 18. What is skewness in a dataset?\n",
    "Skewness measures asymmetry in data distribution:\n",
    "- **Positive skew**: Right tail longer\n",
    "- **Negative skew**: Left tail longer\n",
    "- **Zero skew**: Symmetrical distribution\n",
    "\n",
    "## 19. What does it mean if a dataset is positively or negatively skewed?\n",
    "- **Positively skewed**: Mean > Median, tail extends to right\n",
    "- **Negatively skewed**: Mean < Median, tail extends to left\n",
    "\n",
    "## 20. Define and explain kurtosis\n",
    "Kurtosis measures the \"tailedness\" of a distribution:\n",
    "- **Leptokurtic**: Heavy tails, peaked (kurtosis > 3)\n",
    "- **Mesokurtic**: Normal tails (kurtosis = 3)\n",
    "- **Platykurtic**: Light tails, flat (kurtosis < 3)\n",
    "\n",
    "## 21. What is the purpose of covariance?\n",
    "Covariance measures how two variables change together:\n",
    "- Indicates direction of linear relationship\n",
    "- Basis for correlation calculation\n",
    "- Used in portfolio theory in finance\n",
    "\n",
    "## 22. What does correlation measure in statistics?\n",
    "Correlation measures the strength and direction of linear relationship between two variables (-1 to 1):\n",
    "- +1: Perfect positive correlation\n",
    "- -1: Perfect negative correlation\n",
    "- 0: No linear correlation\n",
    "\n",
    "## 23. What is the difference between covariance and correlation?\n",
    "- **Covariance**: Measures direction of relationship (unstandardized)\n",
    "- **Correlation**: Measures strength and direction (standardized to -1 to 1)\n",
    "- Correlation is dimensionless, covariance units are product of variable units\n",
    "\n",
    "## 24. What are some real-world applications of statistics?\n",
    "- **Medicine**: Clinical trials, epidemiology\n",
    "- **Business**: Market research, quality control\n",
    "- **Finance**: Risk assessment, portfolio management\n",
    "- **Sports**: Player performance analysis\n",
    "- **Government**: Census, policy evaluation\n",
    "- **Science**: Experimental design, data analysis\n",
    "- **Technology**: Machine learning, AI algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572655b",
   "metadata": {},
   "source": [
    "# Practical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9623a44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 4.857142857142857, Median: 5, Mode: [5]\n",
      "Variance: 4.57, Standard Deviation: 2.14\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.groupby(strata_col).apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.sample(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x), n_per_stratum)))\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m data = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRandom sample:\u001b[39m\u001b[33m\"\u001b[39m, random_sample(data, \u001b[32m10\u001b[39m))\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# Statistical Calculations and Python Implementations\n",
    "\n",
    "## 1. How to calculate mean, median, and mode of a dataset\n",
    "\n",
    "data = [3, 7, 2, 5, 5, 8, 4]\n",
    "\n",
    "# Mean\n",
    "mean = sum(data) / len(data)\n",
    "\n",
    "# Median\n",
    "sorted_data = sorted(data)\n",
    "n = len(sorted_data)\n",
    "median = (sorted_data[n//2] if n % 2 != 0 \n",
    "          else (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2)\n",
    "\n",
    "# Mode\n",
    "from collections import Counter\n",
    "count = Counter(data)\n",
    "mode = [k for k, v in count.items() if v == max(count.values())]\n",
    "\n",
    "print(f\"Mean: {mean}, Median: {median}, Mode: {mode}\")\n",
    "\n",
    "\n",
    "## 2. Python program to compute variance and standard deviation\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def variance_std_dev(data):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    variance = sum((x - mean)**2 for x in data) / (n - 1)  # Sample variance\n",
    "    std_dev = math.sqrt(variance)\n",
    "    return variance, std_dev\n",
    "\n",
    "data = [2, 4, 4, 4, 5, 5, 7, 9]\n",
    "var, std = variance_std_dev(data)\n",
    "print(f\"Variance: {var:.2f}, Standard Deviation: {std:.2f}\")\n",
    "\n",
    "\n",
    "## 3. Dataset classification by measurement type\n",
    "\n",
    "\n",
    "# Example dataset classification\n",
    "nominal = [\"red\", \"blue\", \"green\", \"blue\"]  # Colors (no order)\n",
    "ordinal = [\"low\", \"medium\", \"high\"]  # Ordered categories\n",
    "interval = [20, 25, 30, 35]  # Temperature in °C (no true zero)\n",
    "ratio = [150, 160, 175, 180]  # Heights in cm (true zero exists)\n",
    "\n",
    "\n",
    "## 4. Sampling techniques implementation\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Random sampling\n",
    "def random_sample(data, n):\n",
    "    return random.sample(data, n)\n",
    "\n",
    "# Stratified sampling\n",
    "def stratified_sample(df, strata_col, n_per_stratum):\n",
    "    return df.groupby(strata_col).apply(lambda x: x.sample(min(len(x), n_per_stratum)))\n",
    "\n",
    "# Example usage\n",
    "data = list(range(100))\n",
    "print(\"Random sample:\", random_sample(data, 10))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'value': np.random.randn(100),\n",
    "    'group': np.random.choice(['A', 'B', 'C'], 100)\n",
    "})\n",
    "print(\"Stratified sample:\\n\", stratified_sample(df, 'group', 2))\n",
    "\n",
    "\n",
    "## 5. Python function to calculate range\n",
    "\n",
    "\n",
    "def data_range(data):\n",
    "    return max(data) - min(data)\n",
    "\n",
    "data = [10, 20, 5, 30, 15]\n",
    "print(\"Range:\", data_range(data))\n",
    "\n",
    "\n",
    "## 6. Visualizing skewness with histogram\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create skewed data\n",
    "right_skewed = np.random.gamma(2, 2, 1000)\n",
    "left_skewed = np.max(right_skewed) - right_skewed\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(right_skewed, bins=30)\n",
    "plt.title(\"Right (Positive) Skew\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(left_skewed, bins=30)\n",
    "plt.title(\"Left (Negative) Skew\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## 7. Calculating skewness and kurtosis\n",
    "\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.normal(0, 1, 1000)  # Normal distribution\n",
    "print(f\"Skewness: {skew(data):.2f}\")\n",
    "print(f\"Kurtosis: {kurtosis(data):.2f}\")\n",
    "\n",
    "\n",
    "## 8. Demonstrating positive and negative skewness\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Positive skew (right tail)\n",
    "pos_skew = np.random.exponential(scale=2, size=1000)\n",
    "\n",
    "# Negative skew (left tail)\n",
    "neg_skew = np.max(pos_skew) - pos_skew\n",
    "\n",
    "plt.hist(pos_skew, alpha=0.5, label='Positive Skew')\n",
    "plt.hist(neg_skew, alpha=0.5, label='Negative Skew')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## 9. Calculating covariance between two datasets\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def covariance(x, y):\n",
    "    n = len(x)\n",
    "    mean_x, mean_y = np.mean(x), np.mean(y)\n",
    "    return sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n)) / (n - 1)\n",
    "\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 3, 5, 8, 7]\n",
    "print(\"Covariance:\", covariance(x, y))\n",
    "print(\"Numpy covariance:\", np.cov(x, y)[0, 1])\n",
    "\n",
    "\n",
    "## 10. Calculating correlation coefficient\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def correlation(x, y):\n",
    "    cov = np.cov(x, y)[0, 1]\n",
    "    std_x, std_y = np.std(x), np.std(y)\n",
    "    return cov / (std_x * std_y)\n",
    "\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 3, 5, 8, 7]\n",
    "print(\"Correlation coefficient:\", correlation(x, y))\n",
    "print(\"Pearson correlation:\", pearsonr(x, y)[0])\n",
    "\n",
    "\n",
    "## 11. Creating a scatter plot\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(50)\n",
    "y = 2 * x + np.random.normal(0, 0.1, 50)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('X Variable')\n",
    "plt.ylabel('Y Variable')\n",
    "plt.title('Scatter Plot of X vs Y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## 12. Comparing sampling techniques\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "population = list(range(1000))\n",
    "\n",
    "# Simple random sampling\n",
    "random_sample = random.sample(population, 100)\n",
    "\n",
    "# Systematic sampling\n",
    "k = len(population) // 100\n",
    "start = random.randint(0, k-1)\n",
    "systematic_sample = population[start::k]\n",
    "\n",
    "print(\"Random sample first 10:\", random_sample[:10])\n",
    "print(\"Systematic sample first 10:\", systematic_sample[:10])\n",
    "\n",
    "\n",
    "## 13. Central tendency for grouped data\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# For grouped data (midpoints and frequencies)\n",
    "midpoints = [15, 25, 35, 45]\n",
    "frequencies = [5, 12, 8, 5]\n",
    "\n",
    "# Mean\n",
    "mean = np.average(midpoints, weights=frequencies)\n",
    "\n",
    "# Median class\n",
    "cum_freq = np.cumsum(frequencies)\n",
    "n = sum(frequencies)\n",
    "median_class = next(i for i, cf in enumerate(cum_freq) if cf >= n/2)\n",
    "\n",
    "# Mode (class with highest frequency)\n",
    "mode_class = np.argmax(frequencies)\n",
    "\n",
    "print(f\"Mean: {mean}, Median class: {median_class}, Mode class: {mode_class}\")\n",
    "\n",
    "\n",
    "## 14. Data simulation and analysis\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate data\n",
    "data = np.random.normal(50, 10, 1000)\n",
    "\n",
    "# Central tendency\n",
    "mean, median = np.mean(data), np.median(data)\n",
    "mode = stats.mode(data)[0]\n",
    "\n",
    "# Dispersion\n",
    "std_dev, variance = np.std(data), np.var(data)\n",
    "range_val = np.ptp(data)  # peak-to-peak (max-min)\n",
    "\n",
    "print(f\"Mean: {mean:.2f}, Median: {median:.2f}, Mode: {mode[0]:.2f}\")\n",
    "print(f\"Std Dev: {std_dev:.2f}, Variance: {variance:.2f}, Range: {range_val:.2f}\")\n",
    "\n",
    "\n",
    "## 15. Dataset summary with pandas\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'A': np.random.normal(0, 1, 100),\n",
    "    'B': np.random.uniform(5, 10, 100),\n",
    "    'C': np.random.randint(0, 5, 100)\n",
    "})\n",
    "\n",
    "print(data.describe())\n",
    "\n",
    "\n",
    "## 16. Boxplot for spread and outliers\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.normal(0, 1, 100)\n",
    "data = np.append(data, [3, -3])  # Add outliers\n",
    "\n",
    "sns.boxplot(data=data)\n",
    "plt.title('Boxplot Showing Spread and Outliers')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## 17. Calculating IQR\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "data = np.random.normal(0, 1, 100)\n",
    "q1, q3 = np.percentile(data, [25, 75])\n",
    "iqr = q3 - q1\n",
    "\n",
    "print(f\"IQR: {iqr:.2f}\")\n",
    "print(f\"Scipy IQR: {stats.iqr(data):.2f}\")\n",
    "\n",
    "\n",
    "## 18. Z-score normalization\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def z_score_normalize(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    return [(x - mean) / std for x in data]\n",
    "\n",
    "data = [10, 20, 30, 40, 50]\n",
    "normalized = z_score_normalize(data)\n",
    "print(\"Original:\", data)\n",
    "print(\"Normalized:\", normalized)\n",
    "\n",
    "\n",
    "## 19. Comparing datasets by standard deviation\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data1 = np.random.normal(0, 1, 1000)\n",
    "data2 = np.random.normal(0, 2, 1000)\n",
    "\n",
    "std1, std2 = np.std(data1), np.std(data2)\n",
    "print(f\"Dataset 1 SD: {std1:.2f}, Dataset 2 SD: {std2:.2f}\")\n",
    "print(f\"Dataset 2 is {std2/std1:.1f} times more variable than Dataset 1\")\n",
    "\n",
    "\n",
    "## 20. Covariance heatmap\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame(np.random.randn(100, 5), columns=list('ABCDE'))\n",
    "cov_matrix = data.cov()\n",
    "\n",
    "sns.heatmap(cov_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Covariance Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## 21. Correlation matrix with seaborn\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame(np.random.randn(100, 5), columns=list('ABCDE'))\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## 22. Variance and standard deviation implementation\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def compute_var_std(data):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    variance = sum((x - mean)**2 for x in data) / (n - 1)\n",
    "    std_dev = math.sqrt(variance)\n",
    "    return variance, std_dev\n",
    "\n",
    "data = [2, 4, 4, 4, 5, 5, 7, 9]\n",
    "variance, std_dev = compute_var_std(data)\n",
    "print(f\"Variance: {variance:.2f}, Standard Deviation: {std_dev:.2f}\")\n",
    "\n",
    "\n",
    "## 23. Visualizing skewness and kurtosis\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Create different distributions\n",
    "normal = np.random.normal(0, 1, 1000)\n",
    "right_skew = np.random.exponential(1, 1000)\n",
    "high_kurtosis = np.random.laplace(0, 1, 1000)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "titles = [\n",
    "    f\"Normal (Skew: {skew(normal):.2f}, Kurtosis: {kurtosis(normal):.2f})\",\n",
    "    f\"Right Skew (Skew: {skew(right_skew):.2f}, Kurtosis: {kurtosis(right_skew):.2f})\",\n",
    "    f\"High Kurtosis (Skew: {skew(high_kurtosis):.2f}, Kurtosis: {kurtosis(high_kurtosis):.2f})\"\n",
    "]\n",
    "\n",
    "for ax, data, title in zip(axes, [normal, right_skew, high_kurtosis], titles):\n",
    "    ax.hist(data, bins=30)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## 24. Pearson and Spearman correlation\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Create correlated data\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = x + np.random.normal(0, 1, 100)\n",
    "\n",
    "# Calculate correlations\n",
    "pearson_corr, _ = pearsonr(x, y)\n",
    "spearman_corr, _ = spearmanr(x, y)\n",
    "\n",
    "print(f\"Pearson correlation: {pearson_corr:.3f}\")\n",
    "print(f\"Spearman correlation: {spearman_corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f5ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
